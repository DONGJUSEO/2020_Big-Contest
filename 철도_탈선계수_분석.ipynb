{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1z_EvbbfUqUsyvqoKJFV8wlUPzy_aDEPa",
      "authorship_tag": "ABX9TyP2dlsV+GrlyqptS4vt+o6j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DONGJUSEO/2020_Big-Contest/blob/master/%EC%B2%A0%EB%8F%84_%ED%83%88%EC%84%A0%EA%B3%84%EC%88%98_%EB%B6%84%EC%84%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델링\n",
        "### 1) AutoML\n",
        "### 2) ExtraTrees\n",
        "### 3) TabNet\n",
        "### 4) LSTM (Darts)"
      ],
      "metadata": {
        "id": "sExIWDdEIOmd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## AutoML\n",
        "\n",
        "'''\n",
        "y_train.columns = ['YL_M1_B1_W1']\n",
        "data = pd.concat([x_train, y_train], axis = 1)\n",
        "\n",
        "reg_test_1 = setup(data = data,\n",
        "                   target = 'YL_M1_B1_W1',\n",
        "                   train_size = 0.90,\n",
        "                   data_split_shuffle = False,\n",
        "                   normalize = False,\n",
        "                   #normalize_method = 'minmax',\n",
        "                   #polynomial_features = True,\n",
        "                   #polynomial_degree = 2,\n",
        "                   #remove_multicollinearity = True,\n",
        "                   #multicollinearity_threshol = 0.9,\n",
        "                   #feature_selection = True,\n",
        "                   #feature_selection_method = 'classic',\n",
        "                   session_id = 20230806,\n",
        "                   verbose = True)\n",
        "\n",
        "# 모델 탐색\n",
        "gc.collect()\n",
        "start = time.time()\n",
        "best = compare_models(sort = 'mape', fold = 3, n_select = 1, include = ['xgboost', 'rf'])\n",
        "print(\"time :\", time.time() - start)\n",
        "\n",
        "# 모델 생성\n",
        "automl_model = create_model(best)\n",
        "\n",
        "# 하이퍼파라미터 튜닝\n",
        "gc.collect()\n",
        "start = time.time()\n",
        "tuned_automl_model = tune_model(automl_model, optimize = 'MAPE', n_iter = 3)\n",
        "print(\"time :\", time.time() - start)\n",
        "\n",
        "# 모델링\n",
        "automl_pred = predict_model(tuned_automl_model, data = x_valid.reset_index(drop = True))\n",
        "automl_pred = pd.DataFrame(automl_pred.loc[:, 'prediction_label'])\n",
        "y_valid = y_valid.reset_index(drop = True)\n",
        "y_valid.columns = ['prediction_label']\n",
        "\n",
        "MAPE(y_valid, automl_pred)\n",
        "'''"
      ],
      "metadata": {
        "id": "lwkQuFSPIjt0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ExtraTrees\n",
        "\n",
        "'''\n",
        "#!pip install bayesian-optimization\n",
        "from bayes_opt import BayesianOptimization\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "gc.collect()\n",
        "\n",
        "# 하이퍼파라미터 튜닝\n",
        "def et_bo(n_estimators, max_depth, min_samples_split, min_samples_leaf) :\n",
        "    et_params = {'n_estimators' : int(round(n_estimators)),\n",
        "                 'max_depth' : int(round(max_depth)),\n",
        "                 'min_samples_split' : int(round(min_samples_split)),\n",
        "                 'min_samples_leaf' : int(round(min_samples_leaf)),}\n",
        "    et_model = ExtraTreesRegressor(**et_params)\n",
        "    et_model.fit(x_train, y_train)\n",
        "    et_pred = et_model.predict(x_valid)\n",
        "    score = -mean_absolute_error(y_valid, et_pred) # maximize이므로 음수로 지정\n",
        "    return score\n",
        "et_parameter_bounds = {'n_estimators' : (200, 900),\n",
        "                       'max_depth' : (3, 9),\n",
        "                       'min_samples_split' : (2, 9),\n",
        "                       'min_samples_leaf' : (1, 5)\n",
        "                       ,}\n",
        "bo_et = BayesianOptimization(f = et_bo, pbounds = et_parameter_bounds, random_state = 20230807)\n",
        "bo_et.maximize(init_points = 2, n_iter = 10)\n",
        "max_params = bo_et.max['params']\n",
        "max_params['n_estimators'] = int(max_params['n_estimators'])\n",
        "max_params['max_depth'] = int(max_params['max_depth'])\n",
        "max_params['min_samples_split'] = int(max_params['min_samples_split'])\n",
        "max_params['min_samples_leaf'] = int(max_params['min_samples_leaf'])\n",
        "print(max_params)\n",
        "\n",
        "# 모델링\n",
        "et_model = ExtraTreesRegressor(n_estimators = max_params['n_estimators'],\n",
        "                                max_depth = max_params['max_depth'],\n",
        "                                min_samples_split = max_params['min_samples_split'],\n",
        "                                min_samples_leaf = max_params['min_samples_leaf'],\n",
        "                                random_state = 20230807)\n",
        "et_model.fit(x_train, y_train)\n",
        "et_pred = et_model.predict(x_valid)\n",
        "\n",
        "def MAPE(y_test, y_pred) :\n",
        "\treturn np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "MAPE(y_valid.reset_index(drop = True), et_pred)\n",
        "\n",
        "# 예측 시각화\n",
        "plt.figure(figsize = (16, 8))\n",
        "plt.plot(y_valid.reset_index(drop = True), color = 'red', alpha = 0.75)\n",
        "plt.plot(et_pred, color = 'green', alpha = 1)\n",
        "plt.title('ExtraTrees', fontsize = 20)\n",
        "plt.xlabel('Index', fontsize = 12)\n",
        "plt.ylabel('Duration', fontsize = 12)\n",
        "plt.legend(['Test Set', 'Prediction'])\n",
        "plt.show()\n",
        "\n",
        "# Feature Importance\n",
        "importance = et_model.feature_importances_\n",
        "feature = x_train.columns\n",
        "et_f_i = pd.DataFrame()\n",
        "et_f_i['feature'] = feature\n",
        "et_f_i['importances'] = importance\n",
        "et_f_i.sort_values('importances', ascending = False, inplace = True)\n",
        "et_f_i.reset_index(drop = True, inplace = True)\n",
        "sns.barplot(data = et_f_i, x = 'importances', y = 'feature')\n",
        "plt.show()\n",
        "'''"
      ],
      "metadata": {
        "id": "1JxOUW6zJNiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TabNet\n",
        "\n",
        "'''\n",
        "gc.collect()\n",
        "\n",
        "# 필수 전처리 작업\n",
        "clf_x_train = x_train.to_numpy()\n",
        "clf_y_train = y_train.to_numpy().reshape(-1, 1)\n",
        "clf_x_valid = x_valid.to_numpy()\n",
        "clf_y_valid = y_valid.to_numpy().reshape(-1, 1)\n",
        "\n",
        "# 하이퍼파라미터 튜닝\n",
        "def clf_bo(n_d, gamma, momentum) :\n",
        "    clf_params = {'n_d' : int(round(n_d)),\n",
        "                  'gamma' : gamma,\n",
        "                  'momentum' : momentum,}\n",
        "    clf_model = TabNetRegressor(**clf_params,\n",
        "                                optimizer_params = dict(lr = 0.01),\n",
        "                                scheduler_params = {'step_size' : 10, 'gamma' : 0.9},\n",
        "                                scheduler_fn = torch.optim.lr_scheduler.StepLR,\n",
        "                                mask_type = 'entmax')\n",
        "    clf_model.fit(clf_x_train, clf_y_train,\n",
        "                  max_epochs = 200,\n",
        "                  patience = 10,\n",
        "                  batch_size = 128)\n",
        "    clf_pred = clf_model.predict(clf_x_valid)\n",
        "    score = -mean_absolute_error(clf_y_valid, clf_pred)\n",
        "    return score\n",
        "clf_parameter_bounds = {'n_d' : (8, 32),\n",
        "                        'gamma' : (1, 2),\n",
        "                        'momentum' : (0.01, 0.4),}\n",
        "bo_clf = BayesianOptimization(f = clf_bo, pbounds = clf_parameter_bounds, random_state = 20230807)\n",
        "bo_clf.maximize(init_points = 2, n_iter = 10)\n",
        "max_params = bo_clf.max['params']\n",
        "max_params['n_d'] = int(round(max_params['n_d']))\n",
        "max_params['gamma'] = max_params['gamma']\n",
        "max_params['momentum'] = max_params['momentum']\n",
        "print(max_params)\n",
        "\n",
        "# 모델링\n",
        "clf_model = TabNetRegressor(n_d = max_params['n_d'],\n",
        "                            n_a = max_params['n_d'],\n",
        "                            gamma = max_params['gamma'],\n",
        "                            momentum = max_params['momentum'],\n",
        "                            optimizer_params = dict(lr = 0.01),\n",
        "                            scheduler_params = {'step_size' : 10, 'gamma' : 0.9},\n",
        "                            scheduler_fn = torch.optim.lr_scheduler.StepLR,\n",
        "                            mask_type = 'entmax',\n",
        "                            seed = 20230807)\n",
        "clf_model.fit(clf_x_train, clf_y_train,\n",
        "              max_epochs = 200,\n",
        "              patience = 5,\n",
        "              batch_size = 128)\n",
        "clf_pred = clf_model.predict(clf_x_valid)\n",
        "print(mean_absolute_percentage_error(clf_y_valid, clf_pred) * 100)\n",
        "'''"
      ],
      "metadata": {
        "id": "FwU_H2QYYti4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "jbmcULNdpH7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# !pip install darts\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import time\n",
        "# import gc\n",
        "# import matplotlib.pyplot as plt\n",
        "# import seaborn as sns\n",
        "# import torch\n",
        "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "# from darts import TimeSeries\n",
        "# from darts.models import RNNModel, XGBModel\n",
        "# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# from IPython.core.interactiveshell import InteractiveShell\n",
        "# InteractiveShell.ast_node_interactivity = \"all\"\n",
        "# pd.options.display.float_format = '{:.5f}'.format"
      ],
      "metadata": {
        "id": "LyGh_ghscz7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lane_data_s = pd.read_csv('/content/lane_data_s.csv')\n",
        "# lane_data_c = pd.read_csv('/content/lane_data_c.csv')\n",
        "\n",
        "# data_s30 = pd.read_csv('/content/data_s30.csv')\n",
        "# data_s40 = pd.read_csv('/content/data_s40.csv')\n",
        "# data_s50 = pd.read_csv('/content/data_s50.csv')\n",
        "# data_s70 = pd.read_csv('/content/data_s70.csv')\n",
        "# data_s100 = pd.read_csv('/content/data_s100.csv')\n",
        "\n",
        "# data_c30 = pd.read_csv('/content/data_c30.csv')\n",
        "# data_c40 = pd.read_csv('/content/data_c40.csv')\n",
        "# data_c50 = pd.read_csv('/content/data_c50.csv')\n",
        "# data_c70 = pd.read_csv('/content/data_c70.csv')\n",
        "# data_c100 = pd.read_csv('/content/data_c100.csv')"
      ],
      "metadata": {
        "id": "Kv25DIyhLbea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 주성분 분석\n",
        "# from sklearn.decomposition import PCA\n",
        "# pca = PCA(n_components = 2)\n",
        "# printcipalComponents = pca.fit_transform(x)\n",
        "# principalDf = pd.DataFrame(data = printcipalComponents, columns = ['principal component1', 'principal component2'])\n",
        "# pca.explained_variance_ratio_\n",
        "# sum(pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "WtKiR7Ulsz6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "def modelling(data, lane_data, target, length, x) :\n",
        "    # 0 ~ 10000 : 학습데이터, 10001 ~ 11999 : 검증데이터\n",
        "    li = ['Distance', 'YL_M1_B1_W1', 'YR_M1_B1_W1', 'YL_M1_B1_W2', 'YR_M1_B1_W2']\n",
        "    li.remove(target)\n",
        "    m_data = pd.merge(data, lane_data, on = 'Distance')\n",
        "    m_data_train = pd.merge(data, lane_data, on = 'Distance')[0 : 10001]\n",
        "    m_data_train = m_data_train.drop(li, axis = 1)\n",
        "    m_data_test = pd.merge(data, lane_data, on = 'Distance')[10001 : ]\n",
        "    m_data_test = m_data_test.drop(li, axis = 1)\n",
        "\n",
        "    # # 이상치 제거\n",
        "    # q1, q3 = m_data_train[target].quantile(q = 0.25), m_data_train[target].quantile(q = 0.75)\n",
        "    # iqr = q3 - q1\n",
        "    # minimum, maximum = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
        "    # m_data_train = m_data_train[(minimum <= m_data_train[target]) & (m_data_train[target] <= maximum)].reset_index(drop = True)\n",
        "\n",
        "    # # 변수 선택\n",
        "    # corr = m_data_train.corr(method = 'pearson')\n",
        "    # index = corr[(corr[target] >= 0.2) | (corr[target] <= -0.2)].index.tolist()\n",
        "    # m_data_train = m_data_train.loc[:, index]\n",
        "    # m_data_test = m_data_test.loc[:, index]\n",
        "\n",
        "    # 변수 선택\n",
        "    columns = m_data_train.columns.tolist()\n",
        "    columns_li_1 = []\n",
        "    for i in columns :\n",
        "        if (x) in i :\n",
        "            columns_li_1.append(i)\n",
        "    m_data_train = m_data_train.loc[:, columns_li_1]\n",
        "    m_data_test = m_data_test.loc[:, columns_li_1]\n",
        "\n",
        "    # 표준화\n",
        "    y_train = m_data_train[target]\n",
        "    x_train = m_data_train.drop([target], axis = 1)\n",
        "    x_test = m_data_test.drop([target], axis = 1)\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(x_train)\n",
        "    x_train = pd.DataFrame(scaler.transform(x_train))\n",
        "    x_test = pd.DataFrame(scaler.transform(x_test))\n",
        "    scaled_df = pd.concat([x_train, x_test], axis = 0).reset_index(drop = True)\n",
        "\n",
        "    # Time 생성 후 병합\n",
        "    time = pd.DataFrame(pd.date_range('2023-08-13', periods = len(scaled_df), freq = 's'))\n",
        "    time.columns = ['time']\n",
        "    x_train = pd.concat([time.iloc[0 : len(x_train), :], x_train], axis = 1)\n",
        "    y_train = pd.concat([time.iloc[0 : len(y_train), :], y_train], axis = 1)\n",
        "    x_test = pd.concat([time.iloc[len(x_train) - length : , :].reset_index(drop = True), scaled_df.iloc[len(x_train) - length : , :].reset_index(drop = True)], axis = 1)\n",
        "\n",
        "    # 필수 전처리 : 시계열 데이터로 변환\n",
        "    x_train = TimeSeries.from_dataframe(x_train, time_col= \"time\")\n",
        "    y_train = TimeSeries.from_dataframe(y_train, time_col= \"time\")\n",
        "    x_test = TimeSeries.from_dataframe(x_test, time_col= \"time\")\n",
        "\n",
        "    # 모델링\n",
        "    gc.collect()\n",
        "    my_stopper = EarlyStopping(monitor = \"train_loss\",\n",
        "                               patience = 5,\n",
        "    )\n",
        "    pl_trainer_kwargs = {\"callbacks\" : [my_stopper]}\n",
        "\n",
        "    model = RNNModel(input_chunk_length = length,\n",
        "                     output_chunk_length = 1,\n",
        "                     model = 'LSTM',\n",
        "                     dropout = 0,\n",
        "                     batch_size = 32,\n",
        "                     n_epochs = 100,\n",
        "                     optimizer_cls = torch.optim.Adam,\n",
        "                     optimizer_kwargs = {'lr' : 0.001},\n",
        "                     random_state = 20230813,\n",
        "                     pl_trainer_kwargs = pl_trainer_kwargs)\n",
        "    model.fit(series = y_train, future_covariates = x_train)\n",
        "    prediction = model.predict(n = len(x_test) - length, future_covariates = x_test, batch_size = 32)\n",
        "\n",
        "    return prediction\n",
        "'''"
      ],
      "metadata": {
        "id": "5bEUdc9zDhv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# s30_1 = modelling(data_s30, lane_data_s, 'YL_M1_B1_W1', 40, 'W1')\n",
        "# s30_2 = modelling(data_s30, lane_data_s, 'YR_M1_B1_W1', 40, 'W1')\n",
        "# s30_3 = modelling(data_s30, lane_data_s, 'YL_M1_B1_W2', 40, 'W2')\n",
        "# s30_4 = modelling(data_s30, lane_data_s, 'YR_M1_B1_W2', 40, 'W2')\n",
        "# s30_1 = pd.DataFrame(TimeSeries.values(s30_1), columns = ['YL_M1_B1_W1_s30'])\n",
        "# s30_2 = pd.DataFrame(TimeSeries.values(s30_2), columns = ['YR_M1_B1_W1_s30'])\n",
        "# s30_3 = pd.DataFrame(TimeSeries.values(s30_3), columns = ['YL_M1_B1_W2_s30'])\n",
        "# s30_4 = pd.DataFrame(TimeSeries.values(s30_4), columns = ['YR_M1_B1_W2_s30'])\n",
        "# final_data = pd.concat([s30_1, s30_2, s30_3, s30_4], axis = 1)\n",
        "# final_data.to_csv('/content/predict_data_s30.csv')\n",
        "\n",
        "# s40_1 = modelling(data_s40, lane_data_s, 'YL_M1_B1_W1', 40, 'W1')\n",
        "# s40_2 = modelling(data_s40, lane_data_s, 'YR_M1_B1_W1', 40, 'W1')\n",
        "# s40_3 = modelling(data_s40, lane_data_s, 'YL_M1_B1_W2', 40, 'W2')\n",
        "# s40_4 = modelling(data_s40, lane_data_s, 'YR_M1_B1_W2', 40, 'W2')\n",
        "# s40_1 = pd.DataFrame(TimeSeries.values(s40_1), columns = ['YL_M1_B1_W1_s40'])\n",
        "# s40_2 = pd.DataFrame(TimeSeries.values(s40_2), columns = ['YR_M1_B1_W1_s40'])\n",
        "# s40_3 = pd.DataFrame(TimeSeries.values(s40_3), columns = ['YL_M1_B1_W2_s40'])\n",
        "# s40_4 = pd.DataFrame(TimeSeries.values(s40_4), columns = ['YR_M1_B1_W2_s40'])\n",
        "# final_data = pd.concat([s40_1, s40_2, s40_3, s40_4], axis = 1)\n",
        "# final_data.to_csv('/content/predict_data_s40.csv')\n",
        "\n",
        "# s50_1 = modelling(data_s50, lane_data_s, 'YL_M1_B1_W1', 40, 'W1')\n",
        "# s50_2 = modelling(data_s50, lane_data_s, 'YR_M1_B1_W1', 40, 'W1')\n",
        "# s50_3 = modelling(data_s50, lane_data_s, 'YL_M1_B1_W2', 40, 'W2')\n",
        "# s50_4 = modelling(data_s50, lane_data_s, 'YR_M1_B1_W2', 40, 'W2')\n",
        "# s50_1 = pd.DataFrame(TimeSeries.values(s50_1), columns = ['YL_M1_B1_W1_s50'])\n",
        "# s50_2 = pd.DataFrame(TimeSeries.values(s50_2), columns = ['YR_M1_B1_W1_s50'])\n",
        "# s50_3 = pd.DataFrame(TimeSeries.values(s50_3), columns = ['YL_M1_B1_W2_s50'])\n",
        "# s50_4 = pd.DataFrame(TimeSeries.values(s50_4), columns = ['YR_M1_B1_W2_s50'])\n",
        "# final_data = pd.concat([s50_1, s50_2, s50_3, s50_4], axis = 1)\n",
        "# final_data.to_csv('/content/predict_data_s50.csv')\n",
        "\n",
        "# s70_1 = modelling(data_s70, lane_data_s, 'YL_M1_B1_W1', 40, 'W1')\n",
        "# s70_2 = modelling(data_s70, lane_data_s, 'YR_M1_B1_W1', 40, 'W1')\n",
        "# s70_3 = modelling(data_s70, lane_data_s, 'YL_M1_B1_W2', 40, 'W2')\n",
        "# s70_4 = modelling(data_s70, lane_data_s, 'YR_M1_B1_W2', 40, 'W2')\n",
        "# s70_1 = pd.DataFrame(TimeSeries.values(s70_1), columns = ['YL_M1_B1_W1_s70'])\n",
        "# s70_2 = pd.DataFrame(TimeSeries.values(s70_2), columns = ['YR_M1_B1_W1_s70'])\n",
        "# s70_3 = pd.DataFrame(TimeSeries.values(s70_3), columns = ['YL_M1_B1_W2_s70'])\n",
        "# s70_4 = pd.DataFrame(TimeSeries.values(s70_4), columns = ['YR_M1_B1_W2_s70'])\n",
        "# final_data = pd.concat([s70_1, s70_2, s70_3, s70_4], axis = 1)\n",
        "# final_data.to_csv('/content/drive/MyDrive/Colab Notebooks/predict_data_s70.csv')\n",
        "\n",
        "# s100_1 = modelling(data_s100, lane_data_s, 'YL_M1_B1_W1', 40, 'W1')\n",
        "# s100_2 = modelling(data_s100, lane_data_s, 'YR_M1_B1_W1', 40, 'W1')\n",
        "# s100_3 = modelling(data_s100, lane_data_s, 'YL_M1_B1_W2', 40, 'W2')\n",
        "# s100_4 = modelling(data_s100, lane_data_s, 'YR_M1_B1_W2', 40, 'W2')\n",
        "# s100_1 = pd.DataFrame(TimeSeries.values(s100_1), columns = ['YL_M1_B1_W1_s100'])\n",
        "# s100_2 = pd.DataFrame(TimeSeries.values(s100_2), columns = ['YR_M1_B1_W1_s100'])\n",
        "# s100_3 = pd.DataFrame(TimeSeries.values(s100_3), columns = ['YL_M1_B1_W2_s100'])\n",
        "# s100_4 = pd.DataFrame(TimeSeries.values(s100_4), columns = ['YR_M1_B1_W2_s100'])\n",
        "# final_data = pd.concat([s100_1, s100_2, s100_3, s100_4], axis = 1)\n",
        "# final_data.to_csv('/content/drive/MyDrive/Colab Notebooks/predict_data_s100.csv')\n",
        "\n",
        "# c30_1 = modelling(data_c30, lane_data_c, 'YL_M1_B1_W1', 40, 'W1')\n",
        "# c30_2 = modelling(data_c30, lane_data_c, 'YR_M1_B1_W1', 40, 'W1')\n",
        "# c30_3 = modelling(data_c30, lane_data_c, 'YL_M1_B1_W2', 40, 'W2')\n",
        "# c30_4 = modelling(data_c30, lane_data_c, 'YR_M1_B1_W2', 40, 'W2')\n",
        "# c30_1 = pd.DataFrame(TimeSeries.values(c30_1), columns = ['YL_M1_B1_W1_c30'])\n",
        "# c30_2 = pd.DataFrame(TimeSeries.values(c30_2), columns = ['YR_M1_B1_W1_c30'])\n",
        "# c30_3 = pd.DataFrame(TimeSeries.values(c30_3), columns = ['YL_M1_B1_W2_c30'])\n",
        "# c30_4 = pd.DataFrame(TimeSeries.values(c30_4), columns = ['YR_M1_B1_W2_c30'])\n",
        "# final_data = pd.concat([c30_1, c30_2, c30_3, c30_4], axis = 1)\n",
        "# final_data.to_csv('/content/drive/MyDrive/Colab Notebooks/predict_data_c30.csv')\n",
        "\n",
        "# c40_1 = modelling(data_c40, lane_data_c, 'YL_M1_B1_W1', 40, 'W1')\n",
        "# c40_2 = modelling(data_c40, lane_data_c, 'YR_M1_B1_W1', 40, 'W1')\n",
        "# c40_3 = modelling(data_c40, lane_data_c, 'YL_M1_B1_W2', 40, 'W2')\n",
        "# c40_4 = modelling(data_c40, lane_data_c, 'YR_M1_B1_W2', 40, 'W2')\n",
        "# c40_1 = pd.DataFrame(TimeSeries.values(c40_1), columns = ['YL_M1_B1_W1_c40'])\n",
        "# c40_2 = pd.DataFrame(TimeSeries.values(c40_2), columns = ['YR_M1_B1_W1_c40'])\n",
        "# c40_3 = pd.DataFrame(TimeSeries.values(c40_3), columns = ['YL_M1_B1_W2_c40'])\n",
        "# c40_4 = pd.DataFrame(TimeSeries.values(c40_4), columns = ['YR_M1_B1_W2_c40'])\n",
        "# final_data = pd.concat([c40_1, c40_2, c40_3, c40_4], axis = 1)\n",
        "# final_data.to_csv('/content/drive/MyDrive/Colab Notebooks/predict_data_c40.csv')\n",
        "\n",
        "# c50_1 = modelling(data_c50, lane_data_c, 'YL_M1_B1_W1', 40, 'W1')\n",
        "# c50_2 = modelling(data_c50, lane_data_c, 'YR_M1_B1_W1', 40, 'W1')\n",
        "# c50_3 = modelling(data_c50, lane_data_c, 'YL_M1_B1_W2', 40, 'W2')\n",
        "# c50_4 = modelling(data_c50, lane_data_c, 'YR_M1_B1_W2', 40, 'W2')\n",
        "# c50_1 = pd.DataFrame(TimeSeries.values(c50_1), columns = ['YL_M1_B1_W1_c50'])\n",
        "# c50_2 = pd.DataFrame(TimeSeries.values(c50_2), columns = ['YR_M1_B1_W1_c50'])\n",
        "# c50_3 = pd.DataFrame(TimeSeries.values(c50_3), columns = ['YL_M1_B1_W2_c50'])\n",
        "# c50_4 = pd.DataFrame(TimeSeries.values(c50_4), columns = ['YR_M1_B1_W2_c50'])\n",
        "# final_data = pd.concat([c50_1, c50_2, c50_3, c50_4], axis = 1)\n",
        "# final_data.to_csv('/content/drive/MyDrive/Colab Notebooks/predict_data_c50.csv')\n",
        "\n",
        "# c70_1 = modelling(data_c70, lane_data_c, 'YL_M1_B1_W1', 40, 'W1')\n",
        "# c70_2 = modelling(data_c70, lane_data_c, 'YR_M1_B1_W1', 40, 'W1')\n",
        "# c70_3 = modelling(data_c70, lane_data_c, 'YL_M1_B1_W2', 40, 'W2')\n",
        "# c70_4 = modelling(data_c70, lane_data_c, 'YR_M1_B1_W2', 40, 'W2')\n",
        "# c70_1 = pd.DataFrame(TimeSeries.values(c70_1), columns = ['YL_M1_B1_W1_c70'])\n",
        "# c70_2 = pd.DataFrame(TimeSeries.values(c70_2), columns = ['YR_M1_B1_W1_c70'])\n",
        "# c70_3 = pd.DataFrame(TimeSeries.values(c70_3), columns = ['YL_M1_B1_W2_c70'])\n",
        "# c70_4 = pd.DataFrame(TimeSeries.values(c70_4), columns = ['YR_M1_B1_W2_c70'])\n",
        "# final_data = pd.concat([c70_1, c70_2, c70_3, c70_4], axis = 1)\n",
        "# final_data.to_csv('/content/drive/MyDrive/Colab Notebooks/predict_data_c70.csv')\n",
        "\n",
        "# c100_1 = modelling(data_c100, lane_data_c, 'YL_M1_B1_W1', 40, 'W1')\n",
        "# c100_2 = modelling(data_c100, lane_data_c, 'YR_M1_B1_W1', 40, 'W1')\n",
        "# c100_3 = modelling(data_c100, lane_data_c, 'YL_M1_B1_W2', 40, 'W2')\n",
        "# c100_4 = modelling(data_c100, lane_data_c, 'YR_M1_B1_W2', 40, 'W2')\n",
        "# c100_1 = pd.DataFrame(TimeSeries.values(c100_1), columns = ['YL_M1_B1_W1_c100'])\n",
        "# c100_2 = pd.DataFrame(TimeSeries.values(c100_2), columns = ['YR_M1_B1_W1_c100'])\n",
        "# c100_3 = pd.DataFrame(TimeSeries.values(c100_3), columns = ['YL_M1_B1_W2_c100'])\n",
        "# c100_4 = pd.DataFrame(TimeSeries.values(c100_4), columns = ['YR_M1_B1_W2_c100'])\n",
        "# final_data = pd.concat([c100_1, c100_2, c100_3, c100_4], axis = 1)\n",
        "# final_data.to_csv('/content/drive/MyDrive/Colab Notebooks/predict_data_c100.csv')"
      ],
      "metadata": {
        "id": "9RhUvlQxOUZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 최종"
      ],
      "metadata": {
        "id": "P9lOcuUG5PH8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ryBV_-mjcJ2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import scipy.stats as stats\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import xgboost as xgb\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "pd.options.display.float_format = '{:.5f}'.format"
      ],
      "metadata": {
        "id": "yUxTSC6vcFJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lane_data_s = pd.read_csv('/content/lane_data_s.csv')\n",
        "lane_data_c = pd.read_csv('/content/lane_data_c.csv')\n",
        "\n",
        "data_s30 = pd.read_csv('/content/data_s30.csv')\n",
        "data_s40 = pd.read_csv('/content/data_s40.csv')\n",
        "data_s50 = pd.read_csv('/content/data_s50.csv')\n",
        "data_s70 = pd.read_csv('/content/data_s70.csv')\n",
        "data_s100 = pd.read_csv('/content/data_s100.csv')\n",
        "\n",
        "data_c30 = pd.read_csv('/content/data_c30.csv')\n",
        "data_c40 = pd.read_csv('/content/data_c40.csv')\n",
        "data_c50 = pd.read_csv('/content/data_c50.csv')\n",
        "data_c70 = pd.read_csv('/content/data_c70.csv')\n",
        "data_c100 = pd.read_csv('/content/data_c100.csv')"
      ],
      "metadata": {
        "id": "oUTzrUmIcB1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_s30 = pd.read_csv('/content/data_s30.csv')\n",
        "data_s30 = data_s30[0 : 10001]\n",
        "lane_data_s = pd.read_csv('/content/lane_data_s.csv')\n",
        "\n",
        "# 이상치 제거\n",
        "target = 'YL_M1_B1_W1'\n",
        "q1, q3 = data_s30[target].quantile(q = 0.25), data_s30[target].quantile(q = 0.75)\n",
        "iqr = q3 - q1\n",
        "minimum, maximum = q1 - 3 * iqr, q3 + 3 * iqr\n",
        "data_s30 = data_s30[(minimum <= data_s30[target]) & (data_s30[target] <= maximum)].reset_index(drop = True)\n",
        "distance = data_s30.loc[:, 'Distance']\n",
        "\n",
        "# 변수 선택\n",
        "columns_index = data_s30.columns.tolist()\n",
        "columns_li_1 = []\n",
        "for i in columns_index :\n",
        "    if ('W1') in i :\n",
        "        columns_li_1.append(i)\n",
        "columns_li_2 = []\n",
        "for j in columns_li :\n",
        "    if ('L') in j :\n",
        "        columns_li_2.append(j)\n",
        "\n",
        "data_s30 = data_s30.loc[:, columns_li_2]\n",
        "data_s30 = pd.concat([distance, data_s30], axis = 1)\n",
        "data_s30 = pd.merge(data_s30, lane_data_s, on = 'Distance')\n",
        "data_s30 = data_s30.drop(['V_M1_B1_W1_L'], axis = 1)\n",
        "\n",
        "# 다중공선성\n",
        "# data_s30.corr(method = 'pearson')\n",
        "# data_s30.corr(method ='spearman')\n",
        "# from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# vif = pd.DataFrame()\n",
        "# vif[\"VIF Factor\"] = [variance_inflation_factor(data_s30.values, i) for i in range(data_s30.shape[1])]\n",
        "# vif[\"features\"] = data_s30.columns\n",
        "# vif = vif.sort_values(by = \"VIF Factor\", ascending = True)\n",
        "# vif = vif.reset_index().drop(columns = 'index')\n",
        "# vif\n",
        "\n",
        "# Length 설정 (4, 6)\n",
        "target = data_s30[target]\n",
        "for i in range(0, 40) :\n",
        "    print('Length %d : ' % i, stats.pearsonr(target[0 : len(target) - (1 + i)], target[1 + i : ]))"
      ],
      "metadata": {
        "id": "VoTVFtecGFxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def modelling(data, lane_data, target, length) :\n",
        "    train_data = data[0 : 10001]\n",
        "    m_data = pd.merge(data, lane_data, on = 'Distance')\n",
        "    test_data = m_data[10001 : ]\n",
        "\n",
        "    # 이상치 제거\n",
        "    q1, q3 = train_data[target].quantile(q = 0.25), train_data[target].quantile(q = 0.75)\n",
        "    iqr = q3 - q1\n",
        "    minimum, maximum = q1 - 3 * iqr, q3 + 3 * iqr\n",
        "    train_data  = train_data [(minimum <= train_data [target]) & (train_data [target] <= maximum)].reset_index(drop = True)\n",
        "    distance = train_data .loc[:, 'Distance']\n",
        "\n",
        "    # 변수 선택\n",
        "    if target == 'YL_M1_B1_W1' :\n",
        "        x, y, z = 'W1', 'L', 'V_M1_B1_W1_L'\n",
        "    elif target == 'YR_M1_B1_W1' :\n",
        "        x, y, z = 'W1', 'R', 'V_M1_B1_W1_R'\n",
        "    elif target == 'YL_M1_B1_W2' :\n",
        "        x, y, z = 'W2', 'L', 'V_M1_B1_W2_L'\n",
        "    else :\n",
        "        x, y, z = 'W2', 'R', 'V_M1_B1_W2_R'\n",
        "\n",
        "    columns_index = train_data.columns.tolist()\n",
        "    columns_li_1 = []\n",
        "    for i in columns_index :\n",
        "        if (x) in i :\n",
        "            columns_li_1.append(i)\n",
        "    columns_li_2 = []\n",
        "    for j in columns_li_1 :\n",
        "        if (y) in j :\n",
        "            columns_li_2.append(j)\n",
        "\n",
        "    train_data = train_data.loc[:, columns_li_2]\n",
        "    train_data = pd.concat([distance, train_data], axis = 1)\n",
        "    train_data = pd.merge(train_data, lane_data, on = 'Distance')\n",
        "    train_data = train_data.drop(['Distance', z], axis = 1)\n",
        "    train_target = pd.DataFrame(train_data[target])\n",
        "    train_data = train_data.drop(target, axis = 1)\n",
        "\n",
        "    # 스케일링\n",
        "    test_data = test_data.loc[:, train_data.columns.tolist()]\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(train_data)\n",
        "    scaled_train_data = pd.DataFrame(scaler.transform(train_data))\n",
        "    scaled_data = pd.concat([scaled_train_data, train_target], axis = 1)\n",
        "    scaled_test_data = pd.DataFrame(scaler.transform(test_data))\n",
        "\n",
        "    # 모형 구축\n",
        "    def build_model(train_data, target_data, p) :\n",
        "        x = pd.DataFrame()\n",
        "        y = pd.DataFrame()\n",
        "        for i in range(0, len(train_data) - p) :\n",
        "            x_empty = np.array([])\n",
        "            for j in range(0, p) :\n",
        "                empty = np.array(train_data.iloc[i + j, :])\n",
        "                x_empty = np.concatenate((x_empty, empty), axis = 0)\n",
        "            x_empty = pd.DataFrame(x_empty).T\n",
        "            x = x.append(x_empty)\n",
        "            y_empty = pd.DataFrame(np.array(target_data.iloc[i + p, :]))\n",
        "            y = y.append(y_empty.T)\n",
        "        x = x.reset_index(drop = True)\n",
        "        y = y.reset_index(drop = True)\n",
        "        return x, y\n",
        "    train_data, target_data = build_model(scaled_data, train_target, length)\n",
        "\n",
        "    # 모델링\n",
        "    from sklearn.multioutput import MultiOutputRegressor\n",
        "    xgb_model = xgb.XGBRegressor()\n",
        "    xgb_model.fit(train_data, target_data)\n",
        "\n",
        "    # 롤링\n",
        "    scaled_df = pd.concat([scaled_data, scaled_test_data], axis = 0).reset_index(drop = True)\n",
        "    for i in range(0, len(scaled_test_data)) :\n",
        "        part_scaled_df = pd.concat([pd.DataFrame(scaled_df.loc[len(scaled_data) - length + i, :]).T.reset_index(drop = True),\n",
        "                                    pd.DataFrame(scaled_df.loc[len(scaled_data) - length + i + 1, :]).T.reset_index(drop = True),\n",
        "                                    pd.DataFrame(scaled_df.loc[len(scaled_data) - length + i + 2, :]).T.reset_index(drop = True),\n",
        "                                    pd.DataFrame(scaled_df.loc[len(scaled_data) - length + i + 3, :]).T.reset_index(drop = True),], axis = 1)\n",
        "        part_scaled_df.columns = range(0, len(part_scaled_df.columns), 1)\n",
        "        xgb_pred = xgb_model.predict(part_scaled_df)\n",
        "        scaled_df.loc[len(scaled_data) + i, [target]] = xgb_pred.tolist()\n",
        "    scaled_df = scaled_df.loc[len(scaled_data) : , [target]].reset_index(drop = True)\n",
        "\n",
        "    return scaled_df\n",
        "\n",
        "data_1 = modelling(data_s30, lane_data_s, 'YL_M1_B1_W1', 4)\n",
        "data_2 = modelling(data_s30, lane_data_s, 'YR_M1_B1_W1', 4)\n",
        "data_3 = modelling(data_s30, lane_data_s, 'YL_M1_B1_W2', 4)\n",
        "data_4 = modelling(data_s30, lane_data_s, 'YR_M1_B1_W2', 4)\n",
        "final_data = pd.concat([data_1, data_2, data_3, data_4], axis = 1)\n",
        "final_data.to_csv('/content/drive/MyDrive/Colab Notebooks/1.csv')"
      ],
      "metadata": {
        "id": "yay9FFzwWwEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_1 = modelling(data_s30, lane_data_s, 'YL_M1_B1_W1', 4)\n",
        "# data_2 = modelling(data_s30, lane_data_s, 'YR_M1_B1_W1', 4)\n",
        "# data_3 = modelling(data_s30, lane_data_s, 'YL_M1_B1_W2', 4)\n",
        "# data_4 = modelling(data_s30, lane_data_s, 'YR_M1_B1_W2', 4)\n",
        "# final_data = pd.concat([data_1, data_2, data_3, data_4], axis = 1)\n",
        "# final_data.to_csv('/content/drive/MyDrive/Colab Notebooks/1.csv')\n",
        "\n",
        "data_1 = modelling(data_s40, lane_data_s, 'YL_M1_B1_W1', 4)\n",
        "data_2 = modelling(data_s40, lane_data_s, 'YR_M1_B1_W1', 4)\n",
        "data_3 = modelling(data_s40, lane_data_s, 'YL_M1_B1_W2', 4)\n",
        "data_4 = modelling(data_s40, lane_data_s, 'YR_M1_B1_W2', 4)\n",
        "final_data = pd.concat([data_1, data_2, data_3, data_4], axis = 1)\n",
        "final_data.to_csv('/content/drive/MyDrive/Colab Notebooks/2.csv')\n",
        "\n",
        "data_1 = modelling(data_s50, lane_data_s, 'YL_M1_B1_W1', 4)\n",
        "data_2 = modelling(data_s50, lane_data_s, 'YR_M1_B1_W1', 4)\n",
        "data_3 = modelling(data_s50, lane_data_s, 'YL_M1_B1_W2', 4)\n",
        "data_4 = modelling(data_s50, lane_data_s, 'YR_M1_B1_W2', 4)\n",
        "final_data = pd.concat([data_1, data_2, data_3, data_4], axis = 1)\n",
        "final_data.to_csv('/content/drive/MyDrive/Colab Notebooks/3.csv')\n",
        "\n",
        "data_1 = modelling(data_s70, lane_data_s, 'YL_M1_B1_W1', 4)\n",
        "data_2 = modelling(data_s70, lane_data_s, 'YR_M1_B1_W1', 4)\n",
        "data_3 = modelling(data_s70, lane_data_s, 'YL_M1_B1_W2', 4)\n",
        "data_4 = modelling(data_s70, lane_data_s, 'YR_M1_B1_W2', 4)\n",
        "final_data = pd.concat([data_1, data_2, data_3, data_4], axis = 1)\n",
        "final_data.to_csv('/content/drive/MyDrive/Colab Notebooks/4.csv')\n",
        "\n",
        "data_1 = modelling(data_s100, lane_data_s, 'YL_M1_B1_W1', 4)\n",
        "data_2 = modelling(data_s100, lane_data_s, 'YR_M1_B1_W1', 4)\n",
        "data_3 = modelling(data_s100, lane_data_s, 'YL_M1_B1_W2', 4)\n",
        "data_4 = modelling(data_s100, lane_data_s, 'YR_M1_B1_W2', 4)\n",
        "final_data = pd.concat([data_1, data_2, data_3, data_4], axis = 1)\n",
        "final_data.to_csv('/content/drive/MyDrive/Colab Notebooks/5.csv')\n",
        "\n",
        "data_1 = modelling(data_c30, lane_data_c, 'YL_M1_B1_W1', 4)\n",
        "data_2 = modelling(data_c30, lane_data_c, 'YR_M1_B1_W1', 4)\n",
        "data_3 = modelling(data_c30, lane_data_c, 'YL_M1_B1_W2', 4)\n",
        "data_4 = modelling(data_c30, lane_data_c, 'YR_M1_B1_W2', 4)\n",
        "final_data = pd.concat([data_1, data_2, data_3, data_4], axis = 1)\n",
        "final_data.to_csv('/content/drive/MyDrive/Colab Notebooks/6.csv')\n",
        "\n",
        "data_1 = modelling(data_c40, lane_data_c, 'YL_M1_B1_W1', 4)\n",
        "data_2 = modelling(data_c40, lane_data_c, 'YR_M1_B1_W1', 4)\n",
        "data_3 = modelling(data_c40, lane_data_c, 'YL_M1_B1_W2', 4)\n",
        "data_4 = modelling(data_c40, lane_data_c, 'YR_M1_B1_W2', 4)\n",
        "final_data = pd.concat([data_1, data_2, data_3, data_4], axis = 1)\n",
        "final_data.to_csv('/content/drive/MyDrive/Colab Notebooks/7.csv')\n",
        "\n",
        "data_1 = modelling(data_c50, lane_data_c, 'YL_M1_B1_W1', 4)\n",
        "data_2 = modelling(data_c50, lane_data_c, 'YR_M1_B1_W1', 4)\n",
        "data_3 = modelling(data_c50, lane_data_c, 'YL_M1_B1_W2', 4)\n",
        "data_4 = modelling(data_c50, lane_data_c, 'YR_M1_B1_W2', 4)\n",
        "final_data = pd.concat([data_1, data_2, data_3, data_4], axis = 1)\n",
        "final_data.to_csv('/content/drive/MyDrive/Colab Notebooks/8.csv')\n",
        "\n",
        "data_1 = modelling(data_c70, lane_data_c, 'YL_M1_B1_W1', 4)\n",
        "data_2 = modelling(data_c70, lane_data_c, 'YR_M1_B1_W1', 4)\n",
        "data_3 = modelling(data_c70, lane_data_c, 'YL_M1_B1_W2', 4)\n",
        "data_4 = modelling(data_c70, lane_data_c, 'YR_M1_B1_W2', 4)\n",
        "final_data = pd.concat([data_1, data_2, data_3, data_4], axis = 1)\n",
        "final_data.to_csv('/content/drive/MyDrive/Colab Notebooks/9.csv')\n",
        "\n",
        "data_1 = modelling(data_c100, lane_data_c, 'YL_M1_B1_W1', 4)\n",
        "data_2 = modelling(data_c100, lane_data_c, 'YR_M1_B1_W1', 4)\n",
        "data_3 = modelling(data_c100, lane_data_c, 'YL_M1_B1_W2', 4)\n",
        "data_4 = modelling(data_c100, lane_data_c, 'YR_M1_B1_W2', 4)\n",
        "final_data = pd.concat([data_1, data_2, data_3, data_4], axis = 1)\n",
        "final_data.to_csv('/content/drive/MyDrive/Colab Notebooks/10.csv')"
      ],
      "metadata": {
        "id": "8FihhC1xtuBm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "005980_tubME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def modelling(data, lane_data, length) :\n",
        "    m_data = pd.merge(data, lane_data, on = 'Distance')\n",
        "    m_data = m_data.drop('Distance', axis = 1)\n",
        "    m_data_train = m_data[0 : 10001]\n",
        "    m_data_test = m_data[10001 : ]\n",
        "\n",
        "    target = ['YL_M1_B1_W1', 'YR_M1_B1_W1', 'YL_M1_B1_W2', 'YR_M1_B1_W2']\n",
        "    m_data_x_train = m_data_train.drop(target, axis = 1)\n",
        "    m_data_y_train = m_data_train.loc[:, target]\n",
        "    m_data_x_test = m_data_test.drop(target, axis = 1)\n",
        "\n",
        "    # # 스케일링\n",
        "    # scaler = MinMaxScaler()\n",
        "    # scaler.fit(m_data_x_train)\n",
        "    # scaled_data = pd.DataFrame(scaler.transform(m_data_x_train))\n",
        "    # scaled_data_x_train = pd.concat([scaled_data, m_data_y_train], axis = 1)\n",
        "    # scaled_data_x_test = pd.DataFrame(scaler.transform(m_data_x_test))\n",
        "\n",
        "    # 스케일링\n",
        "    m_data_x = pd.concat([m_data_x_train, m_data_x_test], axis = 0)\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(m_data_x)\n",
        "    scaled_data = pd.DataFrame(scaler.transform(m_data_x))\n",
        "    scaled_data_x_train = pd.concat([scaled_data[0 : 10001], m_data_y_train], axis = 1)\n",
        "    scaled_data_x_test = scaled_data[10001 : ]\n",
        "\n",
        "    # 모형 구축\n",
        "    def build_model(train_data, target_data, p) :\n",
        "        x = pd.DataFrame()\n",
        "        y = pd.DataFrame()\n",
        "        for i in range(0, len(train_data) - p) :\n",
        "            x_empty = np.array([])\n",
        "            for j in range(0, p) :\n",
        "                empty = np.array(train_data.iloc[i + j, :])\n",
        "                x_empty = np.concatenate((x_empty, empty), axis = 0)\n",
        "            x_empty = pd.DataFrame(x_empty).T\n",
        "            x = x.append(x_empty)\n",
        "            y_empty = pd.DataFrame(np.array(target_data.iloc[i + p, :]))\n",
        "            y = y.append(y_empty.T)\n",
        "        x = x.reset_index(drop = True)\n",
        "        y = y.reset_index(drop = True)\n",
        "        return x, y\n",
        "    train_data, target_data = build_model(scaled_data_x_train, m_data_y_train, length)\n",
        "\n",
        "    # 모델링\n",
        "    from sklearn.multioutput import MultiOutputRegressor\n",
        "    xgb_multi_model = MultiOutputRegressor(xgb.XGBRegressor())\n",
        "    xgb_multi_model.fit(train_data, target_data)\n",
        "\n",
        "    # 롤링\n",
        "    scaled_df = pd.concat([scaled_data_x_train, scaled_data_x_test], axis = 0).reset_index(drop = True)\n",
        "    for i in range(0, len(m_data_test)) :\n",
        "        part_scaled_df = pd.concat([pd.DataFrame(scaled_df.loc[10001 - length + i, :]).T.reset_index(drop = True),\n",
        "                                    pd.DataFrame(scaled_df.loc[10001 - length + i + 1, :]).T.reset_index(drop = True),\n",
        "                                    pd.DataFrame(scaled_df.loc[10001 - length + i + 2, :]).T.reset_index(drop = True)], axis = 1)\n",
        "        part_scaled_df.columns = range(0, len(part_scaled_df.columns), 1)\n",
        "\n",
        "        pred = xgb_multi_model.predict(part_scaled_df)\n",
        "        scaled_df.loc[10001 + i, ['YL_M1_B1_W1', 'YR_M1_B1_W1', 'YL_M1_B1_W2', 'YR_M1_B1_W2']] = pred.tolist()\n",
        "\n",
        "    return scaled_df"
      ],
      "metadata": {
        "id": "w1SaiUPJXe76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# result_1 = modelling(data_s30, lane_data_s, 3)\n",
        "# result_1 = result_1.loc[10001 : , ['YL_M1_B1_W1', 'YR_M1_B1_W1', 'YL_M1_B1_W2', 'YR_M1_B1_W2']].reset_index(drop = True)\n",
        "# result_1.to_csv('/content/drive/MyDrive/Colab Notebooks/1.csv')\n",
        "\n",
        "# result_2 = modelling(data_s40, lane_data_s, 3)\n",
        "# result_2 = result_2.loc[10001 : , ['YL_M1_B1_W1', 'YR_M1_B1_W1', 'YL_M1_B1_W2', 'YR_M1_B1_W2']].reset_index(drop = True)\n",
        "# result_2.to_csv('/content/drive/MyDrive/Colab Notebooks/2.csv')\n",
        "\n",
        "# result_3 = modelling(data_s50, lane_data_s, 3)\n",
        "# result_3 = result_3.loc[10001 : , ['YL_M1_B1_W1', 'YR_M1_B1_W1', 'YL_M1_B1_W2', 'YR_M1_B1_W2']].reset_index(drop = True)\n",
        "# result_3.to_csv('/content/drive/MyDrive/Colab Notebooks/3.csv')\n",
        "\n",
        "# result_4 = modelling(data_s70, lane_data_s, 3)\n",
        "# result_4 = result_4.loc[10001 : , ['YL_M1_B1_W1', 'YR_M1_B1_W1', 'YL_M1_B1_W2', 'YR_M1_B1_W2']].reset_index(drop = True)\n",
        "# result_4.to_csv('/content/drive/MyDrive/Colab Notebooks/4.csv')\n",
        "\n",
        "# result_5 = modelling(data_s100, lane_data_s, 3)\n",
        "# result_5 = result_5.loc[10001 : , ['YL_M1_B1_W1', 'YR_M1_B1_W1', 'YL_M1_B1_W2', 'YR_M1_B1_W2']].reset_index(drop = True)\n",
        "# result_5.to_csv('/content/drive/MyDrive/Colab Notebooks/5.csv')\n",
        "\n",
        "result_6 = modelling(data_c30, lane_data_c, 3)\n",
        "result_6 = result_6.loc[10001 : , ['YL_M1_B1_W1', 'YR_M1_B1_W1', 'YL_M1_B1_W2', 'YR_M1_B1_W2']].reset_index(drop = True)\n",
        "result_6.to_csv('/content/drive/MyDrive/Colab Notebooks/6.csv')\n",
        "\n",
        "result_7 = modelling(data_c40, lane_data_c, 3)\n",
        "result_7 = result_7.loc[10001 : , ['YL_M1_B1_W1', 'YR_M1_B1_W1', 'YL_M1_B1_W2', 'YR_M1_B1_W2']].reset_index(drop = True)\n",
        "result_7.to_csv('/content/drive/MyDrive/Colab Notebooks/7.csv')\n",
        "\n",
        "result_8 = modelling(data_c50, lane_data_c, 3)\n",
        "result_8 = result_8.loc[10001 : , ['YL_M1_B1_W1', 'YR_M1_B1_W1', 'YL_M1_B1_W2', 'YR_M1_B1_W2']].reset_index(drop = True)\n",
        "result_8.to_csv('/content/drive/MyDrive/Colab Notebooks/8.csv')\n",
        "\n",
        "result_9 = modelling(data_c70, lane_data_c, 3)\n",
        "result_9 = result_9.loc[10001 : , ['YL_M1_B1_W1', 'YR_M1_B1_W1', 'YL_M1_B1_W2', 'YR_M1_B1_W2']].reset_index(drop = True)\n",
        "result_9.to_csv('/content/drive/MyDrive/Colab Notebooks/9.csv')\n",
        "\n",
        "result_10 = modelling(data_c100, lane_data_c, 3)\n",
        "result_10 = result_10.loc[10001 : , ['YL_M1_B1_W1', 'YR_M1_B1_W1', 'YL_M1_B1_W2', 'YR_M1_B1_W2']].reset_index(drop = True)\n",
        "result_10.to_csv('/content/drive/MyDrive/Colab Notebooks/10.csv')"
      ],
      "metadata": {
        "id": "grhhyU9UlNfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def MAPE(y_test, y_pred) :\n",
        "# \treturn np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "# print('1 : ', MAPE(y_valid.iloc[:, 0].values, pd.DataFrame(pred).iloc[:, 0].values))\n",
        "# print('2 : ', MAPE(y_valid.iloc[:, 1].values, pd.DataFrame(pred).iloc[:, 1].values))\n",
        "# print('3 : ', MAPE(y_valid.iloc[:, 2].values, pd.DataFrame(pred).iloc[:, 2].values))\n",
        "# print('4 : ', MAPE(y_valid.iloc[:, 3].values, pd.DataFrame(pred).iloc[:, 3].values))"
      ],
      "metadata": {
        "id": "BCMG_ksYErjv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Da4AbIapYPkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EgUwdlHcYPuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "He_hcEonYQEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yYws1GVqYQOJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Hg74_EZkYQXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\"\n",
        "pd.options.display.float_format = '{:.5f}'.format"
      ],
      "metadata": {
        "id": "c910x1QTYnG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lane_data_s = pd.read_csv('/content/lane_data_s.csv')\n",
        "lane_data_c = pd.read_csv('/content/lane_data_c.csv')\n",
        "\n",
        "data_s30 = pd.read_csv('/content/data_s30.csv')\n",
        "data_s40 = pd.read_csv('/content/data_s40.csv')\n",
        "data_s50 = pd.read_csv('/content/data_s50.csv')\n",
        "data_s70 = pd.read_csv('/content/data_s70.csv')\n",
        "data_s100 = pd.read_csv('/content/data_s100.csv')\n",
        "\n",
        "data_c30 = pd.read_csv('/content/data_c30.csv')\n",
        "data_c40 = pd.read_csv('/content/data_c40.csv')\n",
        "data_c50 = pd.read_csv('/content/data_c50.csv')\n",
        "data_c70 = pd.read_csv('/content/data_c70.csv')\n",
        "data_c100 = pd.read_csv('/content/data_c100.csv')"
      ],
      "metadata": {
        "id": "BE4rCTjDYzKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "\n",
        "data = data_s30\n",
        "lane_data = lane_data_s\n",
        "\n",
        "m_data = pd.merge(data, lane_data, on = 'Distance')\n",
        "m_data = m_data.drop('Distance', axis = 1)\n",
        "m_data_train = m_data[0 : 10001]\n",
        "m_data_test = m_data[10001 : ]\n",
        "\n",
        "# 이상치 제거\n",
        "target = 'YL_M1_B1_W1'\n",
        "q1, q3 = m_data_train[target].quantile(q = 0.25), m_data_train[target].quantile(q = 0.75)\n",
        "iqr = q3 - q1\n",
        "minimum, maximum = q1 - 3 * iqr, q3 + 3 * iqr\n",
        "m_data_train = m_data_train[(minimum <= m_data_train[target]) & (m_data_train[target] <= maximum)].reset_index(drop = True)\n",
        "\n",
        "# 스케일링\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(m_data_train)\n",
        "scaled_m_data_train = scaler.transform(m_data_train)\n",
        "columns = m_data_train.columns\n",
        "scaled_m_data_train = pd.DataFrame(scaled_m_data_train, columns = columns)\n",
        "\n",
        "# 학습모형 구축\n",
        "target = ['YL_M1_B1_W1', 'YR_M1_B1_W1', 'YL_M1_B1_W2', 'YR_M1_B1_W2']\n",
        "scaled_m_data_x_train = scaled_m_data_train.drop(target[1 : ], axis = 1)\n",
        "scaled_m_data_y_train = scaled_m_data_train[target[0]]\n",
        "scaled_m_data_x_train = scaled_m_data_x_train[0 : -1]\n",
        "scaled_m_data_y_train = list(scaled_m_data_y_train[1 : ])\n",
        "\n",
        "# 상수항 추가\n",
        "scaled_m_data_x_train = sm.add_constant(scaled_m_data_x_train, has_constant = 'add')\n",
        "\n",
        "# 데이터 분할\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(scaled_m_data_x_train,\n",
        "                                                      scaled_m_data_y_train,\n",
        "                                                      test_size = 0.05,\n",
        "                                                      shuffle = False,\n",
        "                                                      random_state = 1)\n",
        "# OLS 검정\n",
        "multi_model = sm.OLS(y_train, x_train)\n",
        "fitted_multi_model = multi_model.fit()\n",
        "fitted_multi_model.summary()"
      ],
      "metadata": {
        "id": "KI3EtCdPYz2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = fitted_multi_model.predict(x_valid)"
      ],
      "metadata": {
        "id": "qZKlmqpqeaXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MAPE(y_test, y_pred) :\n",
        "\treturn np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "MAPE(y_valid, pred.reset_index(drop = True))"
      ],
      "metadata": {
        "id": "dB5T4u_CekDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_valid\n",
        "pred.reset_index(drop = True)"
      ],
      "metadata": {
        "id": "YYOvz_8Oj7JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(y_valid)\n",
        "plt.plot(pred.reset_index(drop = True))"
      ],
      "metadata": {
        "id": "4uamPWiVfz6I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}